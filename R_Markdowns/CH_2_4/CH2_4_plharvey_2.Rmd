---
title: "STAT-223: Chapter 2.4 Homework"
author: "Patrick Harvey"
date: "`r Sys.Date()`"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(shiny)
```
### Question 1:

For a random vector:
$\bf\vec{y} \sim{N_{p}} (\vec{\bf{\mu}}\text{, } \mathbf{\Sigma})$, 
let $\bf\vec{z}^{*} = \mathbf{E'\vec{y}}$, where **E** is a square matrix with
the eigenvectors of $\mathbf \Sigma$ stored in the columns.

#### 1.a.: Find the covariance matrix of the random vector $\bf\vec{z}^{*}$
  
  Given the properties of a square matrix with dimensions $p \times p$,
  we know that:
  
  $$\bf A = E \Lambda E^{-1}$$
  
  By definition, **E** is a matrix where each column is an eigenvector of some
  square matrix **A**, therefore:
  \begin{align}
  \text{covmat}
  \bf ( \vec {z} ^ {*} )
  &= \bf A                            \\
  \bf A &= \mathbf{E \Lambda E ^ {-1}}\\
  &= \mathbf{\Lambda}                 \\
  &=
  \begin{bmatrix}
  \lambda_1 & \cdots & 0              \\
  \vdots    & \ddots & \vdots         \\
  0         & \cdots & \lambda_k
  \end{bmatrix}
  \end{align}
  
#### 1.b.: How does the total variability and generalized variability of **z\*** compare to **y**?

  The total variability of a square matrix **A** can be determined by calculating
  the trace of the matrix, such that:
  
  \begin{align}
  \text{tr}
  \bf(A)
  &=
  \sum ^ {p}_{i = 1} {a _ {ii}}       \\
  &=
  a_{11} + a_{22} + \cdots + a_{pp}   \\
  \end{align}
  
  However, this is different for covariance matrices, as we do not need to
  consider the associations between random variables (***y***) such that:
  \begin{align}
  \text{tr}
  \bf (S)
  &=
  \sum ^ {p}_{i=1}{s_{i}^{2}}                   \\
  &=
  s_{1} ^ {2} + s_{2} ^ {2} + \cdots + s_{p}^{2}\\
  \end{align}

  We are given $\bf\vec{z}^{*} = \mathbf{E'\vec{y}}$, where the information
  stored in **z*** is the same as the information in **y**, scaled by the
  pooled standard deviation of the data.
  Strictly speaking, the trace of a column vector does not exist unless it has
  dimensions $1 \times 1$.
  As we are looking at a covariance matrix for some sample of a population, we
  want to standardize the data ($z_{ij}=(y_{ij}-\bar{y}_i)/{\sqrt{s_{ii}}}$)
  and then calculate the determinant of the covariance matrix \\
  ($|S_z| = |\text{var(}z\text{)|}$).
  This relationship can be described as: 
  \begin{align}
  \text{tr} ( \bf \vec{z}^{*} )
  &= \text{tr} (c \mathbf { \vec{y} })   \\
  &=
  c \text{ tr} ( \mathbf { \vec{y} })    \\
  &\propto
  \text{tr} ( \mathbf { \vec{y} })       \\
  \bf R &=
  \text{var} ( \mathbf { \vec{z}^{*}})   \\
  &=
  \text{cor} (\mathbf{ \vec{y} })
  \end{align}
  
#### 1.c.: What did the transformation do to the variables in **y**?

  As described in 1.b. above, this transformation scales the variables in the
  random vector **y** by their corresponding eigenvalues.
  This transformation is equivalent to scaling **y** by the square root of the
  generalized variance of **y**, which is the square root of the determinant of
  the covariance matrix of **y**.
  We know that if **y** is $p \times 1$ and drawn from a multivariate normal
  distribution, then any linear combination will also be distributed according
  to a multivariate normally distribution that is proportional to **y**.
  This proportionality constant is the variance of **y**.
  Additionally, the result ensures that the variables in **y** are linearly
  independent.
  
### Question 2:

  A random vector **y** has 3 variables $\bf y_1, y_2, y_3$ and follows a
  multivariate normal distribution with known $\bf \mu$ and $\bf \Sigma$.
  A scientist collects the 3 variables on *n* samples and transform them into a
  new vector **x** where:
  \begin{align}
  x_1 &= y_1 - y_2\\
  x_2 &= y_1 - y_3\\
  x_3 &= y_2 - y_3
  \end{align}
  
#### 2.a.: What is the transformation matrix (**A**) where **x** = **Ay**?

  We want to know what matrix defines the pairwise differences between the
  variables in **y**. In this case, the covariance matrix of the random vector
  **y** can be used to define this.
  
  Using spectral decomposition on the covariance matrix ($\bf{\Sigma}$) of
  **y**:
  
  ```
  # Given:
  y = matrix(rnorm(3),3)
  x = matrix(data = c(y[1]-y[2], y[1]-y[3], y[2]-y[3]), 3)
  ------------------------------------------------------------------------
  x_d = 1/sqrt(cov(x))                 # Specify covmat(x)^(-1/2)
  
  Sigma = cov(y)
  
  E = matrix(eigen(Sigma)$values,3,3)  # Eigenvalue decomposition of Sigma
  
  solve(E)                             # Uh oh! E is singular.
  ```
  
  
#### 2.b.: The scientist tries to find any outliers in the transformed data, **X**, by using Mahalanobis distance. Write out the formula for Mahalanobis distance for an individual transformed vector (**x**) and the probability distribution it will follow.

  
  Specifically, what we want to define is a transformation that transforms
  covariance matrix of **X** to the identity matrix:
  $$\text{e.g. covmat}(\mathbf{X})^{-1/2} = \mathbf{E}^{-1}\mathbf{\Sigma}^{-1/2}\mathbf{E} = \mathbf{I}$$
  First, let us define the following using a whitening transformation of the
  form:

  \begin{align}
  \text{covmat}(\mathbf{X})^{-1/2} &= \mathbf{X}_{D}\\
  \mathbf{X}_{D} \mathbf{E}^{-1} \mathbf{\Sigma}^{-1/2} \mathbf{E} \mathbf{X}_{D}
  &= \mathbf{I}\\
  \mathbf{\Sigma} ^ {-1/2} &= \mathbf {W} ^ {\mathrm {T} } \mathbf {W}\\
  \mathbf{X}_D\mathbf{E}' &= \mathbf{W}\\
  \therefore \mathbf {X} &= \mathbf{WY}\\ 
  \end{align}

  Pursuing the method above allows us to determine the Mahalanobis distance
  (*d*) as the euclidean norm of the transformation matrix. However, we can also
  describe this measure in the traditional way:
  
  \begin{align}
  d({\mathbf {x}})
  &=
  {\sqrt
    {({\mathbf {x}}-{\mathbf {\mu_{\mathbf{x} }})' \mathbf{\Sigma}^{-1}({\mathbf {x}} - {\mathbf {\mu_{\mathbf{x} }}})}}}\\
  \end{align}
  
  The probability density function of a an observation **x** drawn from a
  multivariate normal distribution can be determined by the Mahalanobis 
  distance *d* as:
  \begin{align}
  P({\mathbf {x}})
  &= \int_{j=1}^{k}{\frac
  {1}
  {\sqrt{2\pi \det(\mathbf{\Sigma})}}
  \exp{\left(-\frac {({\mathbf{x}-{\mathbf{\mu_{x}})'\mathbf{\Sigma}^{-1}
  ({\mathbf{x}}-{\mathbf{\mu_{x}})}}}}{2}\right)}
  d{\mathbf {x}}}\\
  &= \int_{j=1}^{k}{
  \frac
  {1}
  {\sqrt {2\pi \det(\mathbf{\Sigma})}}
  \exp {\left(-\frac{d^{2}}{2}\right)}
  d{\mathbf{x}}}\\
  &=\int_{j=1}^{k}{
  \frac
  {\exp {\left(-\frac{d^{2}}{2}\right)}}
  {\sqrt {\frac{2\pi}{n-1}\left(\sum^{n}_{i=1}{({\mathbf{x}_{i}-{\mathbf{\mu_{x}}}})({\mathbf{x}_{i}-{\mathbf{\mu_{x}}}})'}\right)}}d{\mathbf{x}}}
  \end{align}
  
#### 2.c.: When the scientist tried to calculate the Mahalanobis distance for the sample of *n* random vectors **x**, she received an error. Why canâ€™t she calculate the Mahalanobis distance for **x**? What mistake did she make?

  Fundamentally, the rows of **x** are redundant. Our scientist transformed 
  **y** using linear combinations of its elements, causing the transformed 
  vector **x** to be singular and thus non-invertible.
  In other words, the rows of the *n* vectors she sampled from **X** are linearly
  dependent (i.e. the features exhibit collinearity) and they produce a
  rank-deficient covariance matrix.
  
### Question 3:

The dataset *Lebron.xlxs* has information on all 260 playoff games LeBron James has played during his NBA career. There are 10 variables in the data. First is Playoff game number (Game_num) which you can ignore for now. The other nine variables are a combination of 3 types of shots (worth 1-, 2-, and 3-point shots) and the game statistic (made, missed, and made percent). The names of the variables are in the table below:
  
```
Variable Name           Shot    Point   Value
                        1   	  2   	  3
Game Statistic    Made	Made1   Made2	  Made3
                  Miss	Miss1   Miss2   Miss3
                  %   	Per1    Per2    Per3

The percentage variables are the percentage of successful shots of that type:
e.g. Per = Made / (Made + Missed)
```


#### 3.a.: Calculate the mean for each **y**, **S**, and **R**. You can include a screenshot of the software output instead of creating a table for each as long as it is obvious what each number corresponds to.

  $\mathbf{\bar{y}}$: Each column vector of **Y** is specified in Appendix A: Code below.
```
             y_bar
y_1_bar   6.673077
y_2_bar   2.330769
y_3_bar  72.068462
y_4_bar  10.273077
y_5_bar  10.450000
y_6_bar  49.678462
y_7_bar   1.592308
y_8_bar   3.157692
y_9_bar  30.564615
y_10_bar 31.996154
y_11_bar 32.703846
```
$\mathbf{S}$
```
      Made1         Miss1       Per1         Made2        Miss2        Per2
Made1 14.61471636   1.55256905  35.27266112 -0.39686665   1.92374517  -5.05147015
Miss1  1.55256905   2.60831601 -16.80072468  0.83596674   0.75019305   0.51757648
 Per1 35.27266112 -16.80072468 386.07745708 -8.88787942  -4.07301158 -11.72921473
Made2 -0.39686665   0.83596674  -8.88787942 11.20313335   1.69517375  24.19741016
Miss2  1.92374517   0.75019305  -4.07301158  1.69517375  12.92027027 -26.34316602
 Per2 -5.05147015   0.51757648 -11.72921473 24.19741016 -26.34316602 134.45459222
Made3  0.05925156   0.33228393  -0.73684586  1.62527473  -0.23667954   4.91666766
Miss3  0.04016929   0.43026433  -3.58651322  0.38148203   2.56583012  -5.29465993
 Per3  0.19456489   2.04418176  14.91509593 20.86877339 -18.06471042 103.60641639
       Made3       Miss3        Per3
Made1  0.05925156  0.04016929   0.19456489
Miss1  0.33228393  0.43026433   2.04418176
 Per1 -0.73684586 -3.58651322  14.91509593
Made2  1.62527473  0.38148203  20.86877339
Miss2 -0.23667954  2.56583012 -18.06471042
 Per2  4.91666766 -5.29465993 103.60641639
Made3  1.94897535  0.40430650  23.90405108
Miss3  0.40430650  2.38044253  -9.80057618
 Per3 23.90405108 -9.80057618 514.79727591
```

**R**
```
       Made1       Miss1       Per1        Made2       Miss2.      Per2
Made1  1.00000000  0.22385956  0.44419075 -0.03620370  0.13718741 -0.11395839  
Miss1  0.22385956  1.00000000 -0.67891368  0.16010195  0.14782601  0.03191847  
 Per1  0.44419075 -0.67891368  1.00000000 -0.14457843 -0.05270173 -0.08746712
Made2  0.33454159  0.16010195 -0.14457843  1.00000000  0.13393189  0.62729999
Miss2  0.13718741  0.14782601 -0.05270173  0.13393189  1.00000000 -0.63332556 
 Per2 -0.11395839  0.03191847 -0.08746712  0.62729999 -0.63332556  1.00000000  
Made3  0.00727112  0.14056005 -0.05954490  0.33454159 -0.03981226  0.29723881  
Miss3 -0.01424492  0.14885825 -0.11486937  0.07695814  0.41496828 -0.26841405  
 Per3  0.01427829  0.07153220 -0.01032840  0.26142587 -0.21842854  0.38540723  
       Made3        Miss3       Per3
Made1  0.00727112  -0.01424492  0.01427829
Miss1  0.14056005   0.14885825  0.07153220
 Per1 -0.05954490  -0.11486937 -0.01032840
Made2  1.62527473   0.07695814  0.26142587
Miss2 -0.03981226   0.41496828 -0.21842854
 Per2  0.29723881  -0.26841405  0.38540723
Made3  1.00000000   0.14577495  0.85604948
Miss3  0.14577495   1.00000000 -0.28157497
 Per3  0.85604948  -0.28157497  1.00000000
```

#### 3.cb.: Create a correlation plot for the 9 variables. Do you notice any patterns?

  ![We can see that there are multiple non-linear correlations among these variables. Most of these non-linear relationships appear to be the result of the variables which are linear combinations of the others (e.g. Percentage of shots made for 1, 2, and 3 points)](Pairwise_Correlation.jpeg)

#### 3.c.: Calculate the generalized and total variance using both **S** and **R**

  The total variance is equal to the determinant of a matrix, giving values of:
  \begin{align}
  |\mathbf{S}|&=2357352034\\
  |\mathbf{R}|&=0.003445977
  \end{align}
  The generalized variance is equal to the trace of a matrix, giving values of:
  \begin{align}
  \text{tr}(\mathbf{S})&=1081.005\\
  \text{tr}(\mathbf{R})&=9
  \end{align}

#### 3.d.: For any 3 variables of the same shot value (i.e., Made1, Miss1, Per1), we can find the percentage if we know the number of shots he made and missed in a game. Then why arenâ€™t **S** and **R** singular?

 While each entry for Percentage is a combination of the corresponding Made and Missed shots, each row in Percentage (games) is independent, and each attempt at a shot (Percentage point values) is independent.
  Additionally, for a fixed Percentage (e.g. 45%) ratio of Made to Missed shots cannot be reduced to a single combination of numerical values, as the total number of shots is not defined in the data.

#### 3.e.: For each game in the data, create two new variables:
Points_Scored = 1\*Made1 + 2\*Made2 + 3\*Made3

Points_Missed = 1\*Miss1   + 2\*Miss2   + 3\*Miss3

##### i: Create a scatterplot for the two new variables. Does there appear to be any games that are bivariate outliers?

![Comparing the bivariate distribution of points made versus points missed, we can see that there are several games that appear to be outliers.](made_missed.jpeg)

##### ii: Calculate the Mahalanobis distance for each game just using the two new variables. Create a scatterplot with Game_num on the horizontal axis and the distance on the vertical axis. Do any of the games have a large distance? 

![The plot above shows the thirteen games where the Mahalanobis Distances are outliers (>= 6.45) in red.](mahala_point_game.jpeg)

#### 3.f.: Check to see if the 6 shot variables (Made and Missed for 1-, 2-, 3-point shots) are multivariate Normal.

For any test that you conduct, make sure to state the hypotheses, p-values, and conclusions. If the hypotheses are the same for multiple tests, you just need to state them once.

##### i: Creating the univariate QQ plots and univariate tests of Normality

  ![](made_miss_qq.jpeg)

##### ii: Multivariate QQ plot

  ![](Chi_Square.jpeg)
  ![](Adj_Chi_Square.jpeg)

##### iii: A multivariate Normality test.

Tests and summary statistics:
```
         Test       HZ p-value MVN
Henze-Zirkler 2.054344       0  NO 

         n      Mean   Std.Dev Median  Min   Max  25th   75th     Skew  Kurt
Made1  260  6.673077  3.822920    6.0  0.0  18.0  4.00  9.000  0.57168 -0.062048
Miss1  260  2.330769  1.615028    2.0  0.0   8.0  1.00  3.000  0.55351  0.043386
 Per1  260 72.068462 19.648854   75.0  0.0 100.0 62.50 84.875 -0.89977  1.299539
Made2  260 10.273077  3.347108   10.0  2.0  20.0  8.00 12.000  0.28541  0.066433
Miss2  260 10.450000  3.594478   10.0  2.0  24.0  8.00 12.000  0.63436  1.014660
 Per2  260 49.678462 11.595456   50.0 11.1  84.6 42.25 57.100  0.05483  0.247913
Made3  260  1.592308  1.396057    1.0  0.0   7.0  1.00  2.000  1.01920  0.925842
Miss3  260  3.157692  1.542868    3.0  0.0   8.0  2.00  4.000  0.27679 -0.032410
 Per3  260 30.564615 22.689144   33.3  0.0 100.0 16.70 50.000  0.36809 -0.134002
```
#### 3.g.: A test to check if the remaining 3 variables (Per1, Per2, Per3) are MVN is strongly rejected. State what might be the reason they arenâ€™t MVN and recommend a potential solution (be specific and state why the solution may work). You donâ€™t need to do any actual calculations for this part!

#### Appendix A: Code
```
################################################################################
library(dplyr)
library(ggplot2)
library(GGally)
library(MVN)
library(readxl)
################################################################################
# y_i ~ N_3(0, 1)
y = matrix(rnorm(3),3)
# x_i = [y_i1 - y_i2, y_i1 - y_i3, y_i2 - y_i3]^T
x = matrix(data = c(y[1]-y[2], y[1]-y[3], y[2]-y[3]), 3)
# Specify covmat(x)^(-1/2)
x_d = 1/sqrt(cov(x))
# 3x3 Identity matrix
I = diag(1,3,3,FALSE)
# Sigma = covmat(y)
Sigma = cov(y)
# Eigenvalue decomposition of Sigma
E = matrix(eigen(Sigma)$values,3,3)
# Uh oh! It is singular:
solve(E)

y_1 = matrix(rnorm(3),3,1)
y_2 = matrix(rnorm(3),3,1)
y_3 = matrix(rnorm(3),3,1)
y = matrix(c(y_1, y_2, y_3),3,3)

y_d = 1/sqrt(cov(y))
Sigma = cov(y)
E = matrix(eigen(Sigma)$values,3,3)
solve(E)
################################################################################
data <- read_xlsx('data/Lebron.xlsx') %>% select(-Game_num)

y_1 <- matrix(data$Made1, length(data$Made1), 1)            # 1-point made
y_1_bar <- mean(y_1) # 6.673077
y_2 <- matrix(data$Miss1, length(data$Miss1), 1)            # 1-point miss
y_2_bar <- mean(y_2) # 2.330769
y_3 <- matrix(data$Per1, length(data$Per1), 1)              # (% 1)
y_3_bar <- mean(y_3) # 72.06846
y_4 <- matrix(data$Made2, length(data$Made2), 1)            # 2-point made
y_4_bar <- mean(y_4) # 10.27308
y_5 <- matrix(data$Miss2, length(data$Miss2), 1)            # 2-point miss
y_5_bar <- mean(y_5) # 10.45
y_6 <- matrix(data$Per2, length(data$Per2), 1)              # (% 2)
y_6_bar <- mean(y_6) # 49.67846
y_7 <- matrix(data$Made3, length(data$Made3), 1)            # 3-point made
y_7_bar <- mean(y_7) # 1.592308
y_8 <- matrix(data$Miss3, length(data$Miss3), 1)            # 3-point miss
y_8_bar <- mean(y_8) # 3.157692
y_9 <- matrix(data$Per3, length(data$Per3), 1)              # (% 3)
y_9_bar <- mean(y_9) # 30.56462

Y <- matrix(data=c(y_1,y_2,y_3,y_4,y_5,y_6,y_7,y_8,y_9),
               length(y_1), 9)                            # Made/Miss (Pcts)
colnames(Y) <- c('Made1','Miss1','Per1',
                    'Made2','Miss2','Per2',
                    'Made3','Miss3','Per3')
################################################################################
y_10<- matrix(data=c(data$Made1+2*data$Made2+3*data$Made3),
              length(data$Made1), 1)                        # Made Points
y_10_bar <- mean(y_10) # 31.99615

y_11<- matrix(data=c(data$Miss1+2*data$Miss2+3*data$Miss3),
              length(data$Miss1), 1)                        # Miss Points
y_11_bar <- mean(y_11) # 32.70385

Y_2  <- matrix(data=c(y_10,y_11),
               length(y_1), 2)                            # Made/Miss (Points)
colnames(Y_2) <- c('MadePts','MissPts')
################################################################################
y_bar_tab <- matrix(rbind(y_1_bar,  y_2_bar,  y_3_bar,
                          y_4_bar,  y_5_bar,  y_6_bar,
                          y_7_bar,  y_8_bar,  y_9_bar,
                          y_10_bar, y_11_bar), 11, 1)
colnames(y_bar_tab) <- c('y_bar')
rownames(y_bar_tab) <- c('y_1_bar',  'y_2_bar',  'y_3_bar',
                         'y_4_bar',  'y_5_bar',  'y_6_bar',
                         'y_7_bar',  'y_8_bar',  'y_9_bar',
                         'y_10_bar', 'y_11_bar')
as.table(y_bar_tab)
################################################################################
as.table(cov(Y))
################################################################################
mvn(Y)
cor(Y, method = c("spearman"))
result <- mvn(Y, mvnTest = "hz", multivariateOutlierMethod = "quan")
result <- mvn(Y, mvnTest = "hz", multivariateOutlierMethod = "adj")
################################################################################
ggpairs(data.frame(Y))
################################################################################
det(cov(Y))        # 2357352034.000000000
sum(diag(cov(Y)))  #       1081.005000000
################################################################################
det(cor(Y))        #          0.003445977
sum(diag(cor(Y)))  #          9.000000000
################################################################################
```
