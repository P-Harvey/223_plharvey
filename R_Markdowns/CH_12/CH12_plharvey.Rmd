---
title: "CH_12_HW_plharvey"
output: html_notebook
date: "2023-02-14"
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(readxl, tidyverse, GGally, factoextra, gridExtra, cluster, ggrepel, NbClust, shiny)

data <- read_delim('telescope.csv', delim=',', show_col_types = FALSE)

p = ncol(data)

pc_S <- prcomp(x = data)

pc_R <- prcomp(x = data,
               scale. = T)

df_S <- data.frame(
  PC = 1:ncol(data),
  
  variance = cov(data) |> 
    eigen() |> 
    pluck('values')
) |> 
  mutate(var_prop = variance/sum(variance),
         cumul_var_prop = cumsum(var_prop))

df_R <- data.frame(
  PC = 1:p,
  
  variance = cor(data) |> 
    eigen() |> 
    pluck('values')
) |> 
  mutate(var_prop = variance/sum(variance),
         cumul_var_prop = cumsum(var_prop))
```

# Question: 1

A sample of 200 was collected with 5 variables. Principal component analysis was performed using the correlation matrix, $\mathbf{R}$, incorrectly and the first two principal components were plotted.

For each plot below, describe what in the plot is wrong and why it shouldn’t be possible if PCA was done correctly. Each plot is a separate example and the answers don’t depend on the other plots shown.

## 1.a:

Given $n = 200; X: {\left\{x_0, x_1, x_2, x_3, x_4\right\}}$, let's assume that some error was made during calculation of the correlation matrix $\textbf{R}$. We can see that the variation of Principal Component 2 is greater than the variation in Principal Component 1.

Specifically, the data appear to be centered on:

\begin{align}
  \bar{PC}_{1} &= 0\\
  \bar{PC}_{2} &= 0.
\end{align}

However, the ranges appear to be:

\begin{align}
  PC_{1} &: (-3,3)\\ 
  PC_{2} &: (-5,5). 
\end{align}

One apparent error is that we would expect the output of PCA using a correlation
matrix to be bound on the interval $[0,1]$, 
as it should be a standardized combination of the two variables
($\mathbf{z}_i = \mathbf{e}_i'\mathbf{x}$).

We would want to use a covariance matrix since the variable scales appear to be
similar between the two variables (presumably between $\pm (3 : 5)$ if the original $\mathbf{y}$ were used instead of $\mathbf{z}$).

## 1.b:

Similarly to the plot in 1.a, 
this plot appears to have been constructed using the unstandardized values.
Additionally, 
this plot also appears to have dissimilar variances between the two variables.
$PC_1$ is mostly in the interval $[-4, 6]$,
while $PC_2$ is in the range $[-2, 2]$.
This suggests that the correlation matrix is appropriate,
but we should standardize the variables ($\mathbf{z}_i = \mathbf{e}_i'\mathbf{x}$) in order to remove the correlation and maximize the variance for principal component analysis. 

## 1.c:

This plot appears to provide a good principal component analysis, however it was made using using the correlation matrix of the original data $\mathbf{y}$.


# Question: 2

The file telescope.csv has 10 measurements on ellipses created by gamma particles in the atmosphere collected by a gamma telescope.  The variables are:

\begin{array}
\\
\textbf{Variable} & \textbf{Description}\\
\text{Length}: &	\text{Length of the ellipse}\\
\text{Width}:	& \text{Width of the ellipse}\\
\text{Size}: & \text{Sum of all pixels in the image created}\\
\text{Conc1}:	& \text{Ratio of the sum of the two highest pixels over size}\\
\text{Conc2}:	& \text{Ratio of the highest pixel over size}\\
\text{Asym}: & \text{Distance from highest pixel to center}\\
\text{Long}: & \text{Third moment along major axis}\\
\text{Trans}: & \text{Third moment along minor axis}\\
\text{Alpha}: & \text{Angle of major axis}\\
\text{Dist}: & \text{Distance from origin to center of ellipse}\\
\end{array}

## 2.a:
\textbf{Prompt:} Calculate the covariance matrix. Should the covariance matrix or correlation matrix be used for PCA? Briefly explain your answer.
```{r summarize_data, include=FALSE}
# summary(data)
```

```{r plot_data, include = FALSE, echo = FALSE, results = 'asis'}
renderPlot(ggpairs(data.frame(cov(data))),
           width = 640,
           height = 640)
```

```{r summ_data, echo=FALSE, results='asis'}
library(DT)
renderDT(cov(data),
         options = list(scrollX = TRUE))
```

\textbf{Response:} Looking at the ranges of variances in the the covariance matrix it does not seem like an appropriate choice for principal component analysis. We have covariance values in the interval $[-17: 4566]$, which is significantly different enough that a correlation matrix would be better suited to remove the linear relationships in the data. 


## 2.b:
\textbf{Prompt:} Create a correlation plot (make sure to include it in your solutions). Do any of the variables only have low correlations with the other 9 variables? If so, which ones?

\textbf{Response:} 

```{r plot_data_2, echo = FALSE, results = 'asis'}
renderPlot(ggpairs(data.frame(cor(data))),
           width = 640,
           height = 640)
```

```{r, echo = FALSE, results = 'asis'}
renderPlot(fviz_pca_biplot(X = pc_R,
                           axes = c(1, 2),
                           geom = "point",
                           alpha.ind = 0.1),
           width = 480,
           height = 640)
```

## 2.c:
\textbf{Prompt:} Regardless of your answer in a), use the correlation matrix to perform PCA. Display the eigenvalues of all 10 PCs.

\textbf{Response:}
```{r q2c, echo=FALSE, results='asis'}
renderTable(eigen(cor(data))$values)
```

This suggests that we should use the first three principal components.

## 2.d:
\textbf{Prompt:} Create a scree plot. How many PCs would you recommend? What percent of the total variance do those PCs retain? Is that a high enough percentage?

\textbf{Response:}
```{r scree_pcr, echo = FALSE, results = 'asis'}
renderPlot(
  fviz_screeplot(X = pc_R,
               choice = "eigenvalue",
               geom = "line",
               linecolor = "steelblue",
               ncp = p) + 
  
  labs(title = "Screeplot using the Correlation Matrix",
       x = "Principal Component") + 
  
  # Average variance line
  geom_hline(yintercept = summary(pc_R)$sdev^2|> mean(),
             color = "darkred"),
  width = 640,
  height = 480
)
```

\textbf{Response:}
```{r summ_data_2, echo=FALSE, results='asis'}
renderDT(df_R,
         options = list(scrollX = TRUE))
```

The scree plot and the cumulative proportion of variance explained by the first three principal components ($\sim 81\% $) suggests that we could use those first three components for our analysis.

## 2.e:
\textbf{Prompt:} How many PCs do you recommend? Briefly justify your answer.

\textbf{Response:} Depending on the application, the first four principal components may be appropriate, as they explain $\sim 88\text{-}93\%$ of the cumulative proportion of the variance. I will only use three.

## 2.f:
\textbf{Prompt:} Display the eigenvectors for the PCs you plan on retaining in part e). Do any of them have an interpretation regarding the original variables? If so, what is(are) the interpretations(s)?

\textbf{Response:}

```{r q2f, echo=FALSE, results='asis'}
d_trm <- data %>% select('asym', 'size', 'trans')
renderTable(eigen(cor(d_trm))$values)
```

## 2.g:
Create plots for the 1st and 2nd PCs and the 1st and 3rd PCs. For both plots, describe any important results displayed in the graph, if any exist.

\textbf{Response:}
```{r, echo = FALSE, results = 'asis'}
renderPlot(ggpairs(data.frame(d_trm)),
           width = 640,
           height = 640)
```

There appears to be a linear relationship between variables ```'size'``` and ```'trans'```.

## 2.h:
Do there appear to be linear dependencies in the original 10 variables? If yes, how many, and what are the combination of variables that form the linear dependencies?

\textbf{Response:}
```{r, echo = FALSE, results = 'asis'}
renderPlot(ggpairs(data.frame(data)),
           width = 640,
           height = 640)
```
We can see that ```'conc2'``` appears to have a linear relationship with ```'conc1'```; ```'width'```, ```'lenfgth'```, and ```'size'``` appear to have a potentially linear relationship.