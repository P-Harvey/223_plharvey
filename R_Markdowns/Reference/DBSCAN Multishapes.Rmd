---
title: 'Ch 15: DBSCAN'
author: "Jacob Martin"
date: "STAT 223"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Loading required packages
pacman::p_load(readxl, tidyverse, GGally, factoextra, 
               cluster, dbscan, stringr)

## Adjusting default ggplot() settings
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = 0.5))
```

## Introduction

DBSCAN is an acronym that stands for 

**D**ensity
**B**ased
**S**patial
**C**lustering of 
**A**pplications with
**N**oise

where *noise* is outliers that don't belong to a true cluster.

We'll be using the fake data from the `factoextra` package called **multishapes** since it has important features to demonstrate the advantage DBSCAN has over k-means clustering.

```{r getting the data}
## Loading in the data from the factoextra package
multishapes <- factoextra::multishapes
head(multishapes)
glimpse(multishapes)

# Changing shape to a factor so there are separate colors per group 
# instead of a gradient
multishapes <- 
  multishapes |> 
  mutate(shape = factor(shape))

```


## Step 1: Plotting the data:

Still should plot the data to start:

```{r plotting the data set}
multishapes |> 
  mutate(noise = shape == 5) |> 
  ggplot(mapping = aes(x = x, 
                       y = y)) + 
  
  geom_point(mapping = aes(color = shape,
                           shape = noise),
             show.legend = F)

```

If we just had the green, teal, and purple clusters, then k-means would be an appropriate method to cluster the data. 

However, the blue, red, and yellow points make clustering with k-means inappropriate since it uses the centroid of a cluster and the red and yellow groups will have the same centroid!

The blue triangles were generated at random and aren't from any of the "true" clusters (considered "noise"), which k-means doesn't have a way of clustering without them since it will include all points in one of the k specified clusters.

## Step 2: Rescaling the data

Even though DBSCAN uses the "density" of the points and not the "distance", we still need to rescale the data before running the algorithm

 Checking the scale of the variables:
 
```{r checking the scales of the variables}
multishapes|> 
  summarize(across(.cols = where(is.numeric),
                   .fns = sd))
```

Since the SD of y is twice the SD of x, we need to rescale the data!

```{r rescaling the data}
### Rescaling the data to have sd of 1 
shape_sc <- 
  multishapes |> 
  
  mutate(across(.cols = where(is.numeric),
                .fns = ~ (. - mean(.))/sd(.))) |> 
  
  # Dropping shape since it isn't a numeric variable
  dplyr::select(-shape)


# Checking the data now
shape_sc|> 
  summarize(across(.cols = where(is.numeric),
                   .fns = sd))
```


### Clustering with k-means

Let's start by naively running the k-means algorithm:

```{r kmeans}
set.seed(1234)
km6 <- kmeans(shape_sc, 
              centers = 6, 
              nstart = 10)

### Plotting the results
data.frame(shape_sc, 
           clusters = factor(km6$cluster)) |>
  
  ggplot(mapping = aes(x = x, 
                       y = y)) + 
  
  geom_point(aes(color = clusters), 
             show.legend = F, 
             size = 1.5,
             alpha = 0.5) + 
  
  labs(title = "Clusters using K-means")
```

So that definitely isn't the best choice of clusters. So let's look at the results using DBSCAN!



## DBSCAN from dbscan package

One advantage of DBSCAN over k-means is that we don't need to specify how many clusters exist in the data to run the algorithm!

Instead, DBSCAN has 2 hyper parameters that we need to specify:

- *min points* = The minimum number of points needed to start a new cluster

- $\varepsilon$ = the distance from a point that the algorithm will "scan" for a new point to join the cluster

Min points typically depends on the number of cases and variables. We'll use min points = 5. So how do we find $\varepsilon$?

### Determining $\varepsilon$

The most common way is to use a kNN distance plot and find the elbow of the plot. The elbow is the distance between close but separate clusters or noise points.

We can use the `kNNdist()` and `kNNdistplot()` functions from the `dbscan` package.

Both have 2 arguments:

- `x = ` the (scaled) data set
- `k = ` the number of nearest neighbors to find the distances for

Let's start by creating a scatterplot to visualize how far the 5th nearest neighbor is for each point:

```{r}
### Creating a plot to visualize how the distance changes for each point
shape_sc |> 
  mutate(kdist = kNNdist(x = dist(shape_sc), 
                         k = 5)) |>
  
  ggplot(mapping = aes(x = x, 
                       y = y)) + 
  
  geom_point(mapping = aes(color = kdist), 
             show.legend = T) + 
  
  scale_color_gradient(low="blue", 
                       high = "red",
                       breaks = seq(0.1, 0.7, by = 0.1)) +
  
  theme(legend.position = "bottom") +
  
  guides(color = guide_colorbar(barwidth = 20,
                                title.vjust = 0.75))
```

The denser the points, the blue-er the points. The red dots are the "noise" points we say from earlier!

Now let's actually create a kNN distance plot:

```{r kNNdistplot}
kNNdistplot(x = dist(shape_sc), 
            k = 5); 

title(main = "KNN Distance Plot with k = 5")
```

The elbow of the plot appears to be around 0.2, so we'll use it for $\varepsilon = 0.2$

```{r dbscan}
# Using the dbscan() function
set.seed(1234)
shapes_db <- dbscan(x = shape_sc, 
                    minPts = 5, 
                    eps = 0.2)

shapes_db
```


Finding what objects are stored in the dbscan output:
```{r} 
names(shapes_db) 
```


Let's plot the results of the dbscan algorithm

```{r}
multishapes |> 
  mutate(cluster = factor(shapes_db$cluster)) |> 

  ggplot(mapping = aes(x = x, 
                       y = y)) + 
  
  geom_point(mapping = aes(color = cluster,
                           shape = factor(shape)), 
             show.legend = F, 
             size = 1) + 
  
  labs(title = "Clusters using DBScan with eps = 0.20 and minPts = 5")
```


Let's create a graph to see how the choice of min points will impact the value of $\varepsilon$

```{r knn dist graph for mulitple choices of minpts}
minpts = c(7, 4, 14)


kNNdist(shape_sc, 
        k = max(minpts),
        all = T) |>  
  
  data.frame() |> 
  
  dplyr::select(contains(as.character(minpts))) |>  
  
  pivot_longer(cols = everything(),
               names_to = "MinPts",
               values_to = "Distance") |>  
  
  group_by(MinPts) |>  
  
  arrange(Distance) |> 
  
  mutate(point = row_number(),
         minPts = factor(str_remove(MinPts, "X"),
                         levels = sort(minpts))) |> 
  
  ggplot(aes(x = point, 
             y = Distance, 
             color = minPts)) + 
  
  geom_line(linewidth = 1) +
  
  labs(title = paste("kNN Distance plot for k =",
                     paste(sort(minpts), 
                           collapse = ", "))) + 
  
  scale_y_continuous(breaks = (0:10)/10,
                     limits = c(0, 0.9),
                     expand = c(0, 0, 0.05, 0))
```