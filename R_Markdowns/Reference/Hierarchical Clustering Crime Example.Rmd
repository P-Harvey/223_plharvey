---
title: "Agglomorative Hierarchical Clustering"
author: "Jacob Martin"
date: "STAT 223"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warnings = F,
                      fig.align = "center")

# Reading in the needed packages
pacman::p_load(readxl, tidyverse, NbClust, dendextend, GGally,
               factoextra, patchwork, cluster, dendextend)


## Updating the default ggplot settings
theme_set(theme_classic())
theme_update(plot.title = element_text(size = 12, 
                                       hjust = 0.5))

# Reading in the crime data and changing the city name col to be the row name
crime <- 
  read_xlsx("~/Projects/STAT_223/223_plharvey/Data/Crime.xlsx") |> 
  column_to_rownames(var = "City")

# Sample size and number of variables
n <- nrow(crime); p <- ncol(crime)
```


## Step 1: Exploratory Analysis


Let's start by looking at a correlation plot:

```{r correlation plot}
ggcorr(data = crime,
       low = "red3",
       high = "blue3",
       mid = "white",
       label = T,
       label_round = 2)
```

Other than burglary and larceny, there aren't a lot of strong correlations between pairs of crimes.

Next, let's create a biplot of the first 2 PCs to see if there are any groups in the data using the `fviz_pca()` in `factoextra`

```{r biplot}
fviz_pca(prcomp(x = crime,
                scale. = T), 
         geom = "text") 
```

With only 16 cities, it's a little difficult to see how many groups there are, but some cities are more similar than others!

First 2 PCs account for 70%, let's look at the 1st and 3rd PC as well

```{r biplot 1 3}
fviz_pca(prcomp(x = crime,
                scale. = T),
         axes = c(1,3),
         geom = "text") 
```

From the biplots looks like there may be 2 or 3 groups


## Hierarchical Clustering 

There are many different functions in R that will do hierarchical clustering. We'll be using `hcut()` function in the `factoextra` package.

There is also `hclust()` in base `R` and `agnes()` in the `cluster` package

`hcut()` needs 2 arguments:

- `x =` a dataset or distance matrix
- `hc_method =` link method to measure distances between clusters
- `k = ` # of clusters (optional). Defaults to 2 if nothing is specified

**Note:** just using `method` will give you a different result, need to use the `hc_method` argument!

**Additional Note:** If x is an non-standarized dataset, make sure to use stand=T

### Using different link methods

#### Single Linkage 

Single linkage will use the closest points between to clusters to measure how far apart each cluster is from one another.

```{r single linkage}
crime_sin <- 
  hcut(x = crime, 
       hc_method = "single", 
       stand = T, 
       k = 2)
```

There are a couple of ways of plotting the dendrogram:

```{r}
plot(as.dendrogram(crime_sin), 
     main = "Dendrogram for Single Linkage")
```

Not the best looking dendrogram

More visually appealing using `fviz_dend()` in `factoextra` package. The downside is that it will show the cluster choice from `hcut()`

```{r}
gg_dend_single <- 
  fviz_dend(crime_sin, 
            main = "Cluster Dendrogram for Single Linkage") 
gg_dend_single
```


`rect = T` adds a red rectangle around the clusters and you can specify the color

```{r}
fviz_dend(crime_sin, 
          main = "Cluster Dendrogram for Single Linkage", 
          rect = T, 
          rect_border = "black") 
```


Control the color of the clusters with `k_colors`:

```{r}
fviz_dend(crime_sin, 
          main = "Cluster Dendrogram for Single Linkage",
          rect = T, 
          rect_border = "red", 
          k_colors = c("blue","orange2")) 
```


Need to rerun the `hcut()` function to choose a different `k` :(

```{r}
hcut(x = crime, 
     hc_method = "single", 
     stand = T,
     k = 3) |> 
fviz_dend(main = "Cluster Dendrogram for Single Linkage")
```

You can get the cluster ID by using $cluster

```{r}
crime_sin$cluster
```

Alternatively, you can get the cluster ID using `cutree()` for multiple choices of `k` without having to rerun the algorithm again:

```{r}
cutree(crime_sin, k=2:5)
```


#### Plot of the data by clusters

We can use `fviz_cluster()` to visualize the results

Since we have more than 2 variables, `fviz_cluster()` will use PCA to display the first 2 PCs (by default)

```{r}
fviz_cluster(crime_sin, 
             geom = "text", 
             show.clust.cent = F, 
             ellipse = F) + 

  labs(title = "Single Linkage Clusters: k = 2") +

  theme_bw() + 
  theme(legend.position = "none") 
```


Can't tell what makes Boston special just from the first 2 PCs, so let's check the 3rd PC again:

```{r}
fviz_cluster(crime_sin, 
             geom = "text", 
             show.clust.cent = F, 
             ellipse = F,
             axes = c(1, 3)) + 
  
  labs(title = "Single Linkage Clusters: k = 2") +
  
  theme_bw() + 
  
  theme(legend.position = "none") + 
  
  scale_y_continuous(expand = expansion(mult = c(0.1, 0.1),
                                          add = c(0, 0)))
```

Whatever crime(s) the third PC represents, Boston has a much higher score than the other cities:

```{r}
prcomp(x = crime,
       scale. = T)
```

Looks like the third PC is mostly Autotheft!


#### Complete Link 

The **complete link** is the opposite of **single link** - It uses the farthest points between the two clusters to determine the distances. 

```{r complete link}
crime_com <- 
  hcut(x = crime, 
       hc_method = "complete", 
       stand = T)

gg_dend_complete <- 
  fviz_dend(crime_com) +
  labs(title = "Complete Linkage")

gg_dend_complete
```

If it's difficult to see where the first large merger distance is, you can add lines to the dendrogram to help see where the distances the clusters were merged starts to become large:

```{r}
fviz_dend(crime_com) +
  
  labs(title = "Cluster Dendrogram with Complete Linkage") + 
  
  geom_hline(yintercept = crime_com$height, 
             linetype="dashed") 
```

Examining the dendrogram from complete linkage, it appears that there are 4 clusters 

Let's look at the results when k = 4

```{r cl4}
crime_com <- 
  hcut(x = crime, 
       hc_method = "complete", 
       stand = T, 
       k = 4)

fviz_dend(crime_com) +
  labs(title = "Dendrogram with Linkage = Complete and k = 4")
```

So what do cities in the same clusters have in common? What separates the 4 cities?


##### Scatterplots to illustrate the clusters

I prefer adding the cluster results to the biplot from earlier to help see where the differences in clusters are

We can specify groups with `fviz_pca()` by using `habillage = group ID`

Let's look at 2 clusters and 4 clusters: 

```{r}
# k = 2
prcomp(x = crime,
       scale. = T) |> 
  
  fviz_pca(geom = "text", 
           col.var = "black",
           habillage = cutree(crime_com,
                              k = 2),
           show.legend = F)


# k = 4
prcomp(x = crime,
       scale. = T) |> 
  
  fviz_pca(axes = c(1, 2),
           geom = "text", 
           col.var = "black",
           habillage = cutree(crime_com,
                              k = 4),
           show.legend = F)
```

You can use hcut with k = multiple cluster # and have it return a matrix with each column corresponding to a choice of the cluster

```{r}
cutree(crime_com, 
       k = c(2, 4)) %>% 
  
  data.frame() %>% 
  
  rename(k2 = X2, 
         k4 = X4) %>% 
  
  rownames_to_column(var = "City") %>%
  
  left_join(y = crime |>  rownames_to_column(var = "City"), 
            by = "City") %>% 
  
  pivot_longer(cols = starts_with("k"),
               names_to = "total_cluster",
               values_to = "clusterID") %>% 

  group_by(total_cluster, clusterID) %>% 
  
  summarize(across(.cols = Murder:Autotheft,
                   .fns = mean)) %>% 
  
  ungroup() %>% 
  
  pivot_longer(cols = Murder:Autotheft,
               names_to = "crime",
               values_to = "rate") %>% 
  
  mutate(clusterID = factor(clusterID),
         crime = as_factor(crime)) %>% 
  
  ggplot(mapping = aes(x = clusterID,
                       y = rate,
                       fill = clusterID)) +
  
  geom_col(position = "dodge",
           color = "black") +
  
  facet_wrap(facets = ~ total_cluster + crime,
             nrow = 2,
             scales = "free") +
  
  theme_test() +
    
  theme(legend.position = "none",
        panel.grid = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) + 
  
  labs(y = "Crime Rate")

```

If there are 2 clusters, it appears that there is a "low crime" group and a "high crime group" with the murder rate being about the same while assaults are much higher for the "high crime group"

If there are 4 clusters:

- A "high murder" cluster (Red)
- A "lower crime but high autotheft" group (green)
- A "high crime other than murder" group (Blue)
- A "low crime" group (purple)

  
  
```{r}
cutree(crime_com, 
       k = c(2,4)) |>  
  
  data.frame() |> 
  
  rownames_to_column(var = "City") |> 
  
  arrange(X2, X4) |> 
  
  #rename(cluster = .) |> 
  
  gt::gt()
```



### Average Link 

The distance apart clusters A & B are using the average link is calculated by:

1) Finding the distance for every $(y_{ij}, y_{kl})$ pair
2) Averaging all pairs of distances calculated in step 1)

$$d^2_{avg} = \frac{1}{ab}\sum_j \sum_l (y_{ij} - y_{kl})^2$$


```{r}
crime_avg <-
  hcut(crime, 
       hc_method = "average", 
       k = 2, 
       stand = T)

# Dendrogram for average linkage
gg_dend_avg <- 
  fviz_dend(crime_avg, 
            main = "Average Link") 

gg_dend_avg
```

This dendrogram doesn't indicate a clear cut off, so it is a little hard to tell how many clusters there are when using average linkage


```{r}
# A plot visualizing the results
fviz_cluster(crime_avg, 
             geom = "text", 
             ellipse = F) +
  theme_bw() + 
  labs(title = "City AHC with Average Link") + 
  theme(legend.position = "none")
```


biplot to see how the clusters are different

```{r}
prcomp(crime,
       scale. = T) |> 
  fviz_pca_biplot(geom = "text", 
                  habillage = cutree(crime_avg, 
                                     k = 2), 
                  col.var = "black") +
  theme_bw() + 
  
  labs(title = "City AHC with Average Link") + 
  
  theme(legend.position = "none")
```


#### Centroid Link 

Using the centroid link, we calculate the average (centroid) for each cluster, then measure how far apart the centroids are:

$$d^2_{cen} = (\bar{y}_i - \bar{y}_k)^2 $$


```{r}
crime_cen <- hcut(crime, 
                  hc_method = "centroid", 
                  k = 2, 
                  stand = T)

fviz_dend(crime_cen, 
          main = "Centroid Link")
```


Well that doesn't look good. The results aren't ultrametric :(

By adding a city into a cluster,  the next merger was closer than the previous one

In presidential terms: BAD!

The centroid link often isn't used because it isn't ultrametric!

```{r}
crime_cen$height |> 
  data.frame() |> 
  mutate(merger_number = row_number(),
         height_change = crime_cen.height - lag(crime_cen.height)) |> 
  
  dplyr::select(merger_number, everything())
```



```{r}
# Visualizing the heights:
tibble(merge_distance = crime_cen$height) %>% 
  
  mutate(merge_order = row_number(),
         bad_mergers = if_else(merge_distance < lag(merge_distance) & 
                                 merge_order != 1,
                               "Bad",
                               "Good")) %>% 
  
  ggplot(mapping = aes(x = merge_order,
                       y = merge_distance)) + 
  
  geom_line() + 
  
  geom_point() + 
  
  geom_point(mapping = aes(alpha = bad_mergers),
             size = 6, 
             color = "red",
             shape = 21,
             show.legend = F)  +
  
  scale_alpha_discrete(range = c(1, 0))
```

**Rule of thumb: don't use the results if they aren't ultrametric**




#### Median Link 

The median link is like the average link, but instead of finding the average distance for all points in Clusters A and B, it finds the median distance:

```{r}
crime_med <- 
  hcut(crime, 
       hc_method = "median", 
       k = 2, 
       stand = T)

fviz_dend(crime_med, 
          main = "Dendrogram with Median Link")
```


Seems we have the same ultrametric problem again....


```{r}
tibble(merge_distance = crime_med$height)  |>  
  
  mutate(merge_order = row_number(),
         bad_mergers = if_else(merge_distance < lag(merge_distance) & merge_order != 1,
                               "Bad",
                               "Good")) |> 
  
  ggplot(mapping = aes(x = merge_order,
                       y = merge_distance)) + 
  
  labs(title = "Median Linkage") +
  
  geom_line() + 
  
  geom_point() + 
  
  geom_point(mapping = aes(alpha = bad_mergers),
             size = 6, 
             color = "red",
             shape = 21,
             show.legend = F)  +
  
  scale_alpha_discrete(range = c(1, 0))
```



#### Wards' Distance 

Wards' distance is a little bit of a misnomer because it doesn't calculate the distances between clusters. 

Instead, for each possible merger between two clusters, it will calculate the Within-Sums-of-Squares (WSS) and choose to merge the two clusters that results in the lowest WSS.

$$d^2_{ward} = \sum_i \sum_j (y_{ij} - \bar{y}_i)^2$$

Typically, Wards' distance gives the "best" clustering result, but not always!

```{r wards linkage}
crime_ward <-
  hcut(crime, 
       hc_method = "ward.D", 
       k = 3, 
       stand = T)

### Dendrogram when k = 3. Looks about right :)
gg_dend_ward <- 
  fviz_dend(crime_ward, 
            main = "Wards' Distance")
gg_dend_ward
```

Using Wards' distance, it looks like there are 3 clusters:

```{r}
# A plot visualizing the results
fviz_cluster(crime_ward, 
             geom = "text", 
             show.clust.cent = F, 
             ellipse = F) + 
  
  labs(title = "Wards' Distance: k = 3") + 
  theme_bw() +
  theme(legend.position = "none")


# biplot to see how the clusters are different
prcomp(crime, 
       scale. = T) |> 
  fviz_pca(geom = "text", 
           col.var = "black",
           habillage = cutree(crime_ward, 
                              k = 3)) + 
  labs(title = "Wards' Distance: k = 3") + 
  theme_bw() +
  theme(legend.position = "none")
```

Let's compare the average crime stats for the 3 clusters:

```{r}
crime |> 
  mutate(cluster = crime_ward$cluster) |> 
  group_by(cluster) |> 
  summarize(across(.cols = Murder:Autotheft,
                   .fns = mean))
```

Group 1 appears to have high Murder and medium rates for the other crimes

Group 2 has the lowest violent crime rate and typical theft rate

Group 3 has the highest non-murder crime rates.


```{r}
crime |> 
  mutate(cluster = crime_ward$cluster) |> 
  group_by(cluster) |> 
  mutate(cluster_order = row_number()) |> 
  ggplot(mapping = aes(x = cluster,
                       y = cluster_order,
                       color = factor(cluster))) +
  
  geom_text(mapping = aes(label = rownames(crime)),
            show.legend = F) + 
  
  labs(y = NULL) + 
  
  scale_x_continuous(breaks = 1:max(crime_ward$cluster)) + 
  
  theme(axis.text.y = element_blank())
```


## Which linkage method is best? 

The choice of link method has a large impact on how the clusters formed. So what is the best choice for these data?

A common way of measuring how well a AHC method clusters the data is to calculate the cophenetic correlation coefficient (often just referred as the cophenetic correlation). It measures how faithfully a dendrogram preserves the pairwise distances between the original unclustered points. 

We can calculate the cophenetic distance using the `cor_cophenetic()` function in the `dendextend` package. 

We need to give it two objects:

- The result of the `hclust()` function
- A distance matrix of the original (rescaled) data

```{r}
cor_cophenetic(crime_ward,
               dist(scale(crime)))
```


Correlation with Wards' distance seems pretty low, but how low is it relative to the other methods?

Let's do it for all of them!


```{r}
crime_coph_cor <- 
  tibble(
    single   = cor_cophenetic(crime_sin, dist(scale(crime))),
    complete = cor_cophenetic(crime_com, dist(scale(crime))),
    average  = cor_cophenetic(crime_avg, dist(scale(crime))),
    centroid = cor_cophenetic(crime_cen, dist(scale(crime))),
    median   = cor_cophenetic(crime_med, dist(scale(crime))),
    ward     = cor_cophenetic(crime_ward, dist(scale(crime)))
  ) |> 
  
  pivot_longer(cols = everything(),
               names_to = "link",
               values_to = "coph_cor") |> 
  
  arrange(-coph_cor)

gt::gt(crime_coph_cor)
```

Looks like *average link* has the highest correlation with *Wards' distance* being the second highest (of the ultrametric methods anyway)!

Let's plot the results!

```{r}
ggplot(data = crime_coph_cor,
       mapping = aes(x = fct_reorder(link, -coph_cor),
                     y = coph_cor)) + 
  
  geom_col(fill = "steelblue",
           color = "black") + 
  
  labs(x = "Link Method",
       y = "Cophenetic Correlation") +
  
  scale_y_continuous(expand = c(0, 0, 0.05, 0),
                     limits = c(0, 1))
```

For the rest of the example, we'll be using the results from the average link.



### Determining the number of clusters 

One advantage of AHC is that you can create a dendrogram to help determine the number of clusters in the data.

But if we need a little help deciding on the number of clusters, the same methods we saw with K-means will work with agglomerative clustering

Let's create a silhouette plot using the `fviz_nbclust()` function:

```{r}
fviz_dend(x = crime_avg,
          k = 3)

fviz_nbclust(x = scale(crime), 
             FUNcluster = hcut, 
             method = "silhouette")
```


Notice we didn't specify the link method. That's because `hcut()` defaults to Wards' Distance for the link method. Thankfully, the `fviz_nbclust()` function allows us to give it the same arguments as the clustering method we are using!

So we can give `fviz_nbclust()` the `hc_method` argument that `hcut()` has!


```{r}
fviz_nbclust(x = scale(crime), 
             FUNcluster = hcut,
             hc_method = "average",
             method = "silhouette")
```


## Comparing Clustering Results 

There are a couple of tools we can use to compare the results of different methods/links used.


### Tanglegram

A tanglegram aligns and plot two dendrograms side by side to compare the end results of each clustering method

Let's compare the two best ultrametric methods for our data: Average and Ward with k = 6

```{r}
# Need to store the results in a dendlist:
dendlist(avg  = as.dendrogram(crime_avg),
         ward = as.dendrogram(crime_ward))  |> 
  
  untangle(method = "step1side")  |>  # Find the best alignment layout
  
  tanglegram()                       # Draw the two dendrograms
```

The tanglegram shows that the difference between Wards' distance and average linkage is when Boston is merged.

Average merges it in the second to last step while ward merges it much earlier with Tucson and Hartford.

There are some alternative options we can use in the tanglegram:

```{r}
dendlist(avg = as.dendrogram(crime_avg),
         med = as.dendrogram(crime_med)) %>%
  
  untangle(method = "step1side") %>% 
  
  tanglegram(
    highlight_distinct_edges = T,       # Turn-off/on dashed lines
    common_subtrees_color_lines = T,    # Turn-off/on line colors
    common_subtrees_color_branches = T  # Color common branches 
  )                       
```

We can measure how dissimilar two dendrograms are with the `entanglement()` function. 

Lower value = similar, higher value = dissimilar

```{r}
dendlist(avg   = as.dendrogram(crime_avg),
         ward  = as.dendrogram(crime_ward)) |> 
  untangle(method = "step1side") |> 
  entanglement()
```

Low value for ward and average, indicating that they are similar, which we saw in the tanglegram!

Let's compare average and median:

```{r}
dendlist(avg   = as.dendrogram(crime_avg),
         med  = as.dendrogram(crime_med))  |> 
  untangle(method = "step1side") |>  
  entanglement()
```

It's higher than ward and average, but still not that high. 



#### Dendrogram Correlation

First let's store our results in a dendlist object

- `dendlist()` creates a list object of multiple dendrograms, which needs the object to be a dendrogram type object
- `as.dendrogram()` turns the `hcut()` object into a dendrogram object

```{r}
crime_dl <- 
  dendlist(sin  = as.dendrogram(crime_sin),
           com  = as.dendrogram(crime_com),
           avg  = as.dendrogram(crime_avg),
           cen  = as.dendrogram(crime_cen),
           med  = as.dendrogram(crime_med),
           ward = as.dendrogram(crime_ward))
```


The function `cor.dendlist()` is used to compute Baker or Cophenetic correlation matrix between a list of trees. 

The value can range between -1 to 1. 

Values near 0 indicate that the two trees are not statistically similar.

Cophenetic correlation matrix
```{r}
cor.dendlist(crime_dl, 
             method = "cophenetic")
```

Let's create a correlation plot for the cophenetic correlation:

```{r}
ggcorr(data = NULL,
       cor_matrix = cor.dendlist(crime_dl, 
                                 method = "cophenetic"),
       high = "blue3",
       low = "red3",
       mid = "white",
       label = T,
       label_round = 2)

```
